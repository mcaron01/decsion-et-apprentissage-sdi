{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Privacy Preserving Machine Learning\n",
    "\n",
    "Course taught by Aurélien Bellet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab session: Gaussian mechanism and Differentially Private SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml  # need sklearn >= 0.22\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>Instructions for submitting your report</font>\n",
    "\n",
    "<font color=red>The deadline for sending your report is **Friday, February 23, 2024 at 23h55**.\n",
    "The report is **by teams of 2 students**, should only consist of a **single** `ipynb` file (Jupyter notebook) with your names indicated clearly, and be **submitted via Moodle** (Décision et apprentissage > TP Privacy-Preserving ML > Rendu TP).\n",
    "\n",
    "The grade will be over 20 points, broken down as follows:\n",
    "- Quality of your answers to the questions: **17** points\n",
    "- Quality of the writing and presentation: **2** points\n",
    "- Absence of any bug: **1** point\n",
    "\n",
    "Penalties: **5** points per 12 hours of extra time; **2** points for any failure to respect the other instructions above.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Gaussian mechanism on simple numeric queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first part, we will implement the Gaussian mechanism and experiment with simple numeric queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be working with a dataset from the US Census (also known as the Adult dataset). You can read about the dataset [here](https://archive.ics.uci.edu/ml/datasets/census+income).\n",
    "\n",
    "The following line loads the dataset from [OpenML](https://www.openml.org/) with the `fetch_openml` method of `sklearn`. The option `as_frame=True` (**requires sklearn version >= 0.22**) loads the dataset in `pandas DataFrame` format: this keeps the attributes in their original form and will be more convenient to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_handle = fetch_openml(name='adult', version=2, as_frame=True)\n",
    "dataset = dataset_handle.frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the dataset, in particular the number of rows (individuals), the number of columns (attributes) and what they represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48842 15\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>226802.0</td>\n",
       "      <td>11th</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>89814.0</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28.0</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>336951.0</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>160323.0</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>7688.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>103497.0</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>34.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>198693.0</td>\n",
       "      <td>10th</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>227026.0</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>63.0</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>104626.0</td>\n",
       "      <td>Prof-school</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>3103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>24.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>369667.0</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>55.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>104996.0</td>\n",
       "      <td>7th-8th</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Craft-repair</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age         workclass    fnlwgt     education  education-num  \\\n",
       "0  25.0           Private  226802.0          11th            7.0   \n",
       "1  38.0           Private   89814.0       HS-grad            9.0   \n",
       "2  28.0         Local-gov  336951.0    Assoc-acdm           12.0   \n",
       "3  44.0           Private  160323.0  Some-college           10.0   \n",
       "4  18.0               NaN  103497.0  Some-college           10.0   \n",
       "5  34.0           Private  198693.0          10th            6.0   \n",
       "6  29.0               NaN  227026.0       HS-grad            9.0   \n",
       "7  63.0  Self-emp-not-inc  104626.0   Prof-school           15.0   \n",
       "8  24.0           Private  369667.0  Some-college           10.0   \n",
       "9  55.0           Private  104996.0       7th-8th            4.0   \n",
       "\n",
       "       marital-status         occupation   relationship   race     sex  \\\n",
       "0       Never-married  Machine-op-inspct      Own-child  Black    Male   \n",
       "1  Married-civ-spouse    Farming-fishing        Husband  White    Male   \n",
       "2  Married-civ-spouse    Protective-serv        Husband  White    Male   \n",
       "3  Married-civ-spouse  Machine-op-inspct        Husband  Black    Male   \n",
       "4       Never-married                NaN      Own-child  White  Female   \n",
       "5       Never-married      Other-service  Not-in-family  White    Male   \n",
       "6       Never-married                NaN      Unmarried  Black    Male   \n",
       "7  Married-civ-spouse     Prof-specialty        Husband  White    Male   \n",
       "8       Never-married      Other-service      Unmarried  White  Female   \n",
       "9  Married-civ-spouse       Craft-repair        Husband  White    Male   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week native-country  class  \n",
       "0           0.0           0.0            40.0  United-States  <=50K  \n",
       "1           0.0           0.0            50.0  United-States  <=50K  \n",
       "2           0.0           0.0            40.0  United-States   >50K  \n",
       "3        7688.0           0.0            40.0  United-States   >50K  \n",
       "4           0.0           0.0            30.0  United-States  <=50K  \n",
       "5           0.0           0.0            30.0  United-States  <=50K  \n",
       "6           0.0           0.0            40.0  United-States  <=50K  \n",
       "7        3103.0           0.0            32.0  United-States   >50K  \n",
       "8           0.0           0.0            40.0  United-States  <=50K  \n",
       "9           0.0           0.0            10.0  United-States  <=50K  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n, d = dataset.shape\n",
    "print(n, d)\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.1 (non-private queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function for each of these types of queries (without privacy) and test them on the dataset:\n",
    "- **Simple count queries**: it takes as input a dataset (`DataFrame`), a categorical attribute (e.g., `\"sex\"`) and a value (e.g., `Male`), and returns how many rows in the dataset have the prescribed attribute value.\n",
    "- **Averaging queries**: it takes as input a dataset and a numeric attribute (e.g., `\"age\"`), and returns the average value of this attribute in the dataset.\n",
    "- **Histogram queries**: it takes as input a dataset and a categorical attribute (e.g., `\"sex\"`), and returns the histogram of counts for this attribute in the dataset (i.e., for each possible value of the attribute, how many rows have this value).\n",
    "\n",
    "Reminder: for a DataFrame `df`, we can access the column corresponding to an attribute `attr` by `df[attr]`. The method `value_counts()` allows to build a histogram of a column.\n",
    "\n",
    "Note: you can use the function `bar_plot_pandas` provided below to draw a bar plot of a pandas Series, which is useful to show histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_plot_pandas(series1, series2=None, label1=\"Series 1\", label2=\"Series 2\", title=\"\"):\n",
    "    '''\n",
    "    Draws a bar plot of one Pandas Series, or two pandas Series with the same index\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    series1 : Series of float\n",
    "        First input\n",
    "    series2 : Series of float, optional\n",
    "        Second input (with same index)\n",
    "    label1 : string, optional\n",
    "        Label for the first series\n",
    "    label2 : string, optional\n",
    "        Label for the second series\n",
    "    title : string, optional\n",
    "        Plot title\n",
    "    '''\n",
    "    if series2 is None:\n",
    "        series1.plot.bar()\n",
    "        plt.legend([label1])\n",
    "    else:\n",
    "        concat_series = pd.DataFrame({label1: series1, label2: series2}).reset_index()\n",
    "        concat_series.plot.bar(x=\"index\", y=[label1, label2], xlabel=\"\", title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_query(df, attribute, value):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Dataset\n",
    "    attribute : string\n",
    "        Name of an attribute with categorical values\n",
    "    value : string or int\n",
    "        Value of attribute to count\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    count : int\n",
    "        The number of records with `attribute=value` in dataset `df`\n",
    "    '''\n",
    "    \n",
    "    # TO COMPLETE\n",
    "    pass # this is just to avoid error while the function is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_query(df, attribute):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Dataset\n",
    "    attribute : string\n",
    "        Name of an attribute with numeric values\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    average : float\n",
    "        The average value of `attribute` in dataset `df`\n",
    "    '''\n",
    "        \n",
    "    # TO COMPLETE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_query(df, attribute):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Dataset\n",
    "    attribute : string\n",
    "        Name of an attribute with categorical values\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    histogram : array or Series of int\n",
    "        The histogram of `attribute`, i.e., the number of times each value of `attribute` appears in `df`\n",
    "    '''\n",
    "    # TO COMPLETE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2 (Gaussian mechanism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the Gaussian mechanism, i.e., a function which takes as input a (non-private) query output, the query's $\\ell_2$ sensitivity, the desired value of $\\epsilon$ and $\\delta$ and a random seed (for reproducibility), and returns a $(\\epsilon,\\delta)$-differentially private estimate of the query. To draw Gaussian noise, check `np.random.normal`. The function should work with queries that output a scalar (like simple count and averaging queries), as well as those that output a vector (like histogram queries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_mechanism(q, s2, eps, delta, random_state=None):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    q : float or array/Series of float\n",
    "        The non-private output of the query\n",
    "    s2 : float\n",
    "        The L2 sensitivity of the query\n",
    "    eps : float\n",
    "        Parameter epsilon of differential privacy\n",
    "    delta : float\n",
    "        Parameter delta of differential privacy\n",
    "    random_state : int, optional (default=None)\n",
    "        Random seed\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    private_q : float or array/Series of float\n",
    "        An (eps,delta)-DP evaluation of the query\n",
    "    '''\n",
    "    \n",
    "    rng = np.random.RandomState(random_state)\n",
    "    \n",
    "    # TO COMPLETE\n",
    "    if hasattr(q, 'shape'): # query output is multi-dimensional\n",
    "        pass\n",
    "    else: # query output is a scalar\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.3 (Private computation of count queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to use the Gaussian mechanism to:\n",
    "- privately count the number of males in the dataset\n",
    "- generate a private histogram of the `\"workclass\"` attribute\n",
    "\n",
    "What is the sensitivity of the query in each case?\n",
    "\n",
    "Run the Gaussian mechanism with different values of $\\epsilon$ and $\\delta$, and compute the relative $\\ell_1$-error with respect to the true (non-private) output and discuss the effect of $\\epsilon$ and $\\delta$ on the utility. Recall that the mechanism is random, so unless you fix the seed you will get a different result at each execution. You may also visually compare the private and non-private histograms using the function `bar_plot_pandas` provided at the beginning of the notebook.\n",
    "\n",
    "Note: you may round the outputs of the private mechanism to make them integers if you like. This can be seen as post-processing and thus preserves DP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_l1_error(q_true, q_est):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    q_true : float or array/Series of float\n",
    "        True value\n",
    "    q_est : float or array/Series of float\n",
    "        Estimated value\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    l1_error : float\n",
    "        The relative L1 error of `q_est` with respect to and `q_true`: ||q_true-q_est||_1 / ||q_true||_1\n",
    "    '''\n",
    "\n",
    "    if not(hasattr(q_true, 'shape')):\n",
    "        return np.abs(q_true - q_est) / np.abs(q_true)\n",
    "    else:\n",
    "        return np.linalg.norm(q_true - q_est, ord=1) / np.linalg.norm(q_true, ord=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill with the list of queries in the format (name, query_function, sensitivity)\n",
    "queries = []\n",
    "\n",
    "# fill with the list of values for epsilon and delta\n",
    "eps_list = []\n",
    "delta_list = []\n",
    "n_runs = 10\n",
    "\n",
    "for name, q, s in queries:\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    error = np.zeros((len(eps_list), len(delta_list), n_runs))\n",
    "    for j, delta in enumerate(delta_list):\n",
    "        for i, eps in enumerate(eps_list):\n",
    "            for r in range(n_runs):\n",
    "                # TO COMPLETE\n",
    "                pass\n",
    "                # error[i, j, r] = \n",
    "    \n",
    "        ax.errorbar(eps_list, error[:, j, :].mean(axis=1), error[:, j, :].std(axis=1),\n",
    "                    label='Gaussian mechanism ($\\delta$=' + \"{:.2e}\".format(delta) + ')')\n",
    "\n",
    "    plt.xlabel(\"$\\epsilon$\")\n",
    "    plt.ylabel(\"$\\ell_1$ error\")\n",
    "    plt.title(\"Query: \" + name)\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Bonus Question</font> (Private computation of  average queries)\n",
    "*(You won't be penalized if you don't answer this question; but can get bonus points if you do)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to use the Gaussian mechanism to privately estimate the average age of people in the dataset.\n",
    "\n",
    "1. Propose and implement simple practical strategies to compute or estimate the sensitivity of this query in the following two scenarios, and discuss the merits and/or drawbacks of your proposals:\n",
    "  - You are the trusted curator: you have access to the raw dataset and would like to release an estimate of the average age of people in the dataset with differential privacy guarantees.\n",
    "  - You are an external data analyst: you do not have access to the raw dataset but only to an API to send queries. You have to convince the trusted curator that the sensitivity you propose is safe.\n",
    "2. Suggest some ideas regarding how we could change a bit the query to get a simple and safe bound on sensitivity, at the expense of possibly introducing some bias in the output. Implement the proposed solution. Hint: the method `clip` from pandas might be useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Differentially Private Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this second part, we will implement and experiment with Differentialy Private Stochastic Gradient Descent (DP-SGD) to privately learn machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You are free to work with any binary classification dataset(s) you like** (you may use more than a single dataset). It is of course possible to work with the US Census dataset used in previous practicals, but you can find other datasets for instance on [OpenML](https://www.openml.org/), [UCI](https://archive.ics.uci.edu/ml/index.php), [sklearn](https://scikit-learn.org/stable/modules/classes.html?highlight=datasets#module-sklearn.datasets).\n",
    "\n",
    "Good candidate datasets should have rather small dimension compared to the number of data points. Examples include US Census in one-hot encoded version (`name='a9a', version=1`), houses (`name='houses', version=2`) and electricity (`name='electricity', version=1`).\n",
    "\n",
    "The code below loads the US Census dataset in one-hot encoded version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48842 123\n"
     ]
    }
   ],
   "source": [
    "X, y = fetch_openml(name='a9a', version=1, return_X_y=True, as_frame=False)\n",
    "n, d = X.shape\n",
    "\n",
    "# convert labels to -1, 1\n",
    "c = np.unique(y)    \n",
    "y[y==c[0]] = -1\n",
    "y[y==c[1]] = 1\n",
    "y = y.astype(float)\n",
    "\n",
    "print(n, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first standardize features, then normalize each point to have unit norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sp.sparse.issparse(X):\n",
    "    scaler = StandardScaler(with_mean=False)\n",
    "else:\n",
    "    scaler = StandardScaler()\n",
    "normalizer = Normalizer()\n",
    "X = normalizer.transform(scaler.fit_transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now split the dataset into a train and a test set. Feel free to adapt the size of the training set to your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39073, 123) (39073,) (9769, 123) (9769,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.80, random_state=42, stratify=y)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "n_train = X_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1 (non-private SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first question, we will implement our own simple version of SGD, as well as define our own sklearn-compatible $\\ell_2$-regularized logistic regression estimator. This will be convenient when we will implement a differentially private version in Question 2.\n",
    "\n",
    "Below, you are given several pieces of code:\n",
    "1. A function `sgd` which implements SGD: it is meant to be generic in the sense that it takes as input a function `obj_and_grad` which computes the value and the gradient of the desired objective function. **This function has missing parts that you need to complete**.\n",
    "2. A function `my_logistic_obj_and_grad` (adapted from [the version from sklearn](https://github.com/scikit-learn/scikit-learn/blob/0fb307bf39bbdacd6ed713c00724f8f871d60370/sklearn/linear_model/_logistic.py#L84)) which computes the value and gradient of the logistic regression problem. You do not need to modify this function.\n",
    "3. A class `MySGDLogisticRegression` which defines a sklearn estimator for logistic regression, where the model is fit using SGD using the previous two functions. You do not need to modify this function.\n",
    "\n",
    "Spend a bit of time to get familiar with the code provided, then complete the missing bits in the `sgd` function. Make sure it works by trying it on the binary classification dataset that you previously loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(X, y, gamma, n_iter, obj_and_grad, theta_init, n_batch=1, freq_obj_eval=10,\n",
    "        n_obj_eval=1000, random_state=None):\n",
    "    \"\"\"Stochastic Gradient Descent (SGD) algorithm\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape (n, d)\n",
    "        The data\n",
    "    y : array, shape (n,)\n",
    "        Binary labels (-1, 1).\n",
    "    gamma : float | callable\n",
    "        The step size. Can be a constant float or a function\n",
    "        that allows to have a variable step size\n",
    "    n_iter : int\n",
    "        The number of iterations\n",
    "    obj_and_grad : callable\n",
    "        A function which takes as a vector of shape (p,), a dataset of shape (n_batch, d)\n",
    "        and a label vector of shape (n_batch,), and returns the objective value and gradient.\n",
    "    theta_init : array, shape (p,)\n",
    "        The initial value for the model parameters\n",
    "    n_batch : int\n",
    "        Size of the mini-batch to use at each iteration of SGD.\n",
    "    freq_obj_eval : int\n",
    "        Specifies the frequency (in number of iterations) at which we compute the objective\n",
    "    n_obj_eval : int\n",
    "        The number of points on which we evaluate the objective\n",
    "    random_state : int\n",
    "        Random seed to make the algorithm deterministic\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    theta : array, shape=(p,)\n",
    "        The final value of the model parameters\n",
    "    obj_list : list of length (n_iter / freq_obj_eval)\n",
    "        A list containing the value of the objective function computed every freq_obj_eval iterations\n",
    "    \"\"\"\n",
    "    \n",
    "    rng = np.random.RandomState(random_state)\n",
    "    n, d = X.shape\n",
    "    p = theta_init.shape[0]\n",
    "    \n",
    "    theta = theta_init.copy()\n",
    "\n",
    "    # if a constant step size was provided, we turn it into a constant function\n",
    "    if not callable(gamma):\n",
    "        def gamma_func(t):\n",
    "            return gamma\n",
    "    else:\n",
    "        gamma_func = gamma\n",
    "    \n",
    "    # list to record the evolution of the objective (for plotting)\n",
    "    obj_list = []\n",
    "    # we draw a fixed subset of points to monitor the objective\n",
    "    idx_eval = rng.randint(0, n, n_obj_eval)\n",
    "\n",
    "    for t in range(n_iter):\n",
    "        if t % freq_obj_eval == 0:\n",
    "            # evaluate objective\n",
    "            obj, _ = obj_and_grad(theta, X[idx_eval, :], y[idx_eval])\n",
    "            obj_list.append(obj)\n",
    "        \n",
    "        # TO COMPLETE\n",
    "        \n",
    "    return theta, obj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model._base import LinearClassifierMixin, SparseCoefMixin, BaseEstimator\n",
    "from sklearn.utils.extmath import log_logistic, safe_sparse_dot\n",
    "from scipy.special import expit\n",
    "from sklearn.utils.validation import check_X_y\n",
    "\n",
    "def _intercept_dot(w, X, y):\n",
    "    \"\"\"Computes y * np.dot(X, w).\n",
    "\n",
    "    It takes into consideration if the intercept should be fit or not.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    w : ndarray, shape (n_features,) or (n_features + 1,)\n",
    "        Coefficient vector.\n",
    "\n",
    "    X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "        Training data.\n",
    "\n",
    "    y : ndarray, shape (n_samples,)\n",
    "        Array of labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    w : ndarray, shape (n_features,)\n",
    "        Coefficient vector without the intercept weight (w[-1]) if the\n",
    "        intercept should be fit. Unchanged otherwise.\n",
    "\n",
    "    X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "        Training data. Unchanged.\n",
    "\n",
    "    yz : float\n",
    "        y * np.dot(X, w).\n",
    "    \"\"\"\n",
    "    c = 0.\n",
    "    if w.size == X.shape[1] + 1:\n",
    "        c = w[-1]\n",
    "        w = w[:-1]\n",
    "\n",
    "    z = safe_sparse_dot(X, w) + c\n",
    "    yz = y * z\n",
    "    return w, c, yz\n",
    "\n",
    "\n",
    "def my_logistic_obj_and_grad(theta, X, y, lamb):\n",
    "    \"\"\"Computes the value and gradient of the objective function of logistic regression defined as:\n",
    "    min (1/n) \\sum_i log_loss(theta;X[i,:],y[i]) + (lamb / 2) \\|w\\|^2,\n",
    "    where theta = w (if no intercept), or theta = [w b] (if intercept)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    theta_init : array, shape (d,) or (d+1,)\n",
    "        The initial value for the model parameters. When an intercept is used, it corresponds to the last entry\n",
    "    X : array, shape (n, d)\n",
    "        The data\n",
    "    y : array, shape (n,)\n",
    "        Binary labels (-1, 1)\n",
    "    lamb : float\n",
    "        The L2 regularization parameter\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    obj : float\n",
    "        The value of the objective function\n",
    "    grad : array, shape (d,) or (d+1,)\n",
    "        The gradient of the objective function\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    grad = np.empty_like(theta)\n",
    "\n",
    "    w, c, yz = _intercept_dot(theta, X, y)\n",
    "\n",
    "    # Logistic loss is the negative of the log of the logistic function\n",
    "    obj = -np.mean(log_logistic(yz)) + .5 * lamb * np.dot(w, w)\n",
    "\n",
    "    z = expit(yz)\n",
    "    z0 = (z - 1) * y\n",
    "\n",
    "    grad[:n_features] = safe_sparse_dot(X.T, z0) / n_samples + lamb * w\n",
    "\n",
    "    # Case where we fit the intercept\n",
    "    if grad.shape[0] > n_features:\n",
    "        grad[-1] = z0.sum() / n_samples\n",
    "    return obj, grad\n",
    "\n",
    "\n",
    "class MySGDLogisticRegression(BaseEstimator, LinearClassifierMixin, SparseCoefMixin):\n",
    "    \"\"\"Our own sklearn estimator for logistic regression defined as:\n",
    "    min (1/n) \\sum_i log_loss(theta;X[i,:],y[i]) + (lamb / 2) \\|w\\|^2,\n",
    "    where theta = [w b]\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    gamma : float | callable\n",
    "        The step size. Can be a constant float or a function\n",
    "        that allows to have a variable step size\n",
    "    n_iter : int\n",
    "        The number of iterations\n",
    "    lamb : float\n",
    "        The L2 regularization parameter\n",
    "    n_batch : int\n",
    "        Size of the mini-batch to use at each iteration of SGD.\n",
    "    freq_obj_eval : int\n",
    "        Specifies the frequency (in number of iterations) at which we compute the objective\n",
    "    n_obj_eval : int\n",
    "        The number of points on which we evaluate the objectuve\n",
    "    random_state : int\n",
    "        Random seed to make the algorithm deterministic\n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    coef_ : (p,)\n",
    "        The weights of the logistic regression model.\n",
    "    intercept_ : (1,)\n",
    "        The intercept term of the logistic regression model.\n",
    "    obj_list_: list of length (n_iter / freq_obj_eval)\n",
    "        A list containing the value of the objective function computed every freq_loss_eval iterations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, gamma, n_iter, lamb=0, n_batch=1, freq_obj_eval=10, n_obj_eval=1000, random_state=None):\n",
    "        self.gamma = gamma\n",
    "        self.n_iter = n_iter\n",
    "        self.lamb = lamb\n",
    "        self.n_batch = n_batch\n",
    "        self.freq_obj_eval = freq_obj_eval\n",
    "        self.n_obj_eval = n_obj_eval\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # WARNING: assumes labels are -1, 1\n",
    "        X, y = check_X_y(X, y, accept_sparse='csr', dtype=[np.float64, np.float32], order=\"C\")\n",
    "        self.classes_ = np.unique(y)    \n",
    "        \n",
    "        p = X.shape[1]\n",
    "        theta_init = np.zeros(p+1) # initialize parameters to zero\n",
    "        # define the function for value and gradient needed by SGD\n",
    "        obj_grad = lambda theta, X, y: my_logistic_obj_and_grad(theta, X, y, lamb=self.lamb)\n",
    "        theta, obj_list = sgd(X, y, self.gamma, self.n_iter, obj_grad, theta_init, self.n_batch,\n",
    "                              self.freq_obj_eval, self.n_obj_eval, self.random_state)\n",
    "        \n",
    "        # save the learned model into the appropriate quantities used by sklearn\n",
    "        self.intercept_ = np.expand_dims(theta[-1], axis=0)\n",
    "        self.coef_ = np.expand_dims(theta[:-1], axis=0)\n",
    "        \n",
    "        # also save list of objective values during optimization for plotting\n",
    "        self.obj_list_ = obj_list\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy 0.7606715119254785\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEHCAYAAAC0pdErAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAZ70lEQVR4nO3df7RddX3m8fdjAkhUCMhlls2PEp1gRUcDHoNWUQakjRZMcbpqoA794TKmikU7YxumU21dnbVU1DWMxdIUUFxiGOTHJOPUACMGlgiYGwiYEKIBlFxjTRhEMFpjwjN/7H1lc7LvuTvh7ntu7n1ea5117/7u797n82XBfdi/vlu2iYiI6PacfhcQERETUwIiIiJqJSAiIqJWAiIiImolICIiolYCIiIiak1vc+eSFgEXA9OAy2x/rGv9h4A/qNTyMmAA+BlwG3BY2X6t7Y+M9n3HHHOMjzvuuDGrPyJislu/fv2jtgfq1qmt5yAkTQO+A5wBDAHrgHNs3z9C/7OAD9o+TZKA59n+qaRDgG8AF9i+s9d3djodDw4Ojuk4IiImM0nrbXfq1rV5imkhsNX2Q7Z3A1cDi3v0PwdYCeDCT8v2Q8pPnuiLiBhHbQbELGBbZXmobNuHpBnAIuC6Sts0SRuAHcDNtu9qr9SIiOjWZkCopm2ko4CzgNttP/arjvZe2wuA2cBCSa+o/RJpqaRBSYM7d+58tjVHRESpzYAYAuZUlmcD20fou4Ty9FI3248DaymOMOrWr7Ddsd0ZGKi9zhIREQegzYBYB8yXNE/SoRQhsLq7k6QjgTcBqyptA5Jmlr8fDrwZeKDFWiMioktrt7na3iPpfOBGittcr7C9SdKycv2lZdezgZts76ps/iLgyvJOqOcA19j+Slu1RkTEvlq7zbUfcptrRMT+6ddtrhERcRBLQERERK0ERERE1EpARERErQRERETUSkBEREStBERERNRKQERERK0ERERE1EpARERErQRERETUSkBEREStBERERNRKQERERK0ERERE1EpARERErQRERETUSkBEREStBERERNRKQERERK1WA0LSIklbJG2VtLxm/YckbSg/GyXtlXS0pDmSvi5ps6RNki5os86IiNhXawEhaRpwCfAW4ATgHEknVPvYvsj2AtsLgAuBW20/BuwB/pPtlwGvBd7XvW1ERLSrzSOIhcBW2w/Z3g1cDSzu0f8cYCWA7R/avrv8/UlgMzCrxVojIqJLmwExC9hWWR5ihD/ykmYAi4DratYdB5wI3DX2JUZExEjaDAjVtHmEvmcBt5enl57egfR8itD4gO0nar9EWippUNLgzp07n1XBERHxtDYDYgiYU1meDWwfoe8SytNLwyQdQhEOV9m+fqQvsb3Cdsd2Z2Bg4FmWHBERw9oMiHXAfEnzJB1KEQKruztJOhJ4E7Cq0ibgcmCz7U+3WGNERIygtYCwvQc4H7iR4iLzNbY3SVomaVml69nATbZ3VdpeD/xH4LTKbbBvbavWiIjYl+yRLgscfDqdjgcHB/tdRkTEQUPSetudunV5kjoiImolICIiolYCIiIiaiUgIiKiVgIiIiJqJSAiIqJWAiIiImolICIiolYCIiIiaiUgIiKiVgIiIiJqJSAiIqJWAiIiImolICIiolYCIiIiaiUgIiKiVgIiIiJqJSAiIqJWAiIiImq1GhCSFknaImmrpOU16z8kaUP52Shpr6Sjy3VXSNohaWObNUZERL3WAkLSNOAS4C3ACcA5kk6o9rF9ke0FthcAFwK32n6sXP15YFFb9UVERG9tHkEsBLbafsj2buBqYHGP/ucAK4cXbN8GPDZy94iIaFObATEL2FZZHirb9iFpBsXRwnUt1hMREfuhzYBQTZtH6HsWcHvl9FLzL5GWShqUNLhz58793TwiIkbQZkAMAXMqy7OB7SP0XULl9NL+sL3Cdsd2Z2Bg4EB2ERERNdoMiHXAfEnzJB1KEQKruztJOhJ4E7CqxVoiImI/tRYQtvcA5wM3ApuBa2xvkrRM0rJK17OBm2zvqm4vaSVwB/BSSUOS3tVWrRERsS/ZI10WKDtIhwH/ATgOmD7cbvujrVZ2ADqdjgcHB/tdRkTEQUPSetudunXT6xq7rAJ+AqwHfjGWhUVExMTVJCBm284DaxERU0yTaxDflPTvWq8kIiImlCZHEG8A/kjSwxSnmATY9itbrSwiIvqqSUC8pfUqIiJiwhn1FJPt7wMzKZ52PguYWbZFRMQkNmpASLoAuAo4tvx8UdL72y4sIiL6q8kppncBJw8/yCbp4xQPsH2mzcIiIqK/mtzFJGBvZXkv9RPxRUTEJNLkCOJzwF2SbiiXfxe4vLWKIiJiQhg1IGx/WtJaittdBfyx7XvaLiwiIvprxICQdITtJ8p3RH+v/AyvO/pA3t0QEREHj15HEF8CzqSYg6k6o5/K5Re3WFdERPTZiAFh+8zy57zxKyciIiaKJs9BfK1JW0RETC69rkE8F5gBHCPpKJ6+tfUI4NfGobaIiOijXtcg3gN8gCIM1vN0QDwBXNJuWRER0W+9rkFcDFws6f2289R0RMQU0+RJ6qckzRxekHSUpPe2V1JEREwETQLi3bYfH16w/WPg3a1VFBERE0KTgHiOpF/NvSRpGnBok51LWiRpi6StkpbXrP+QpA3lZ6OkveWDeaNuGxER7WoSEDcC10g6XdJpwEpgzWgblUFyCcULh04AzpF0QrWP7YtsL7C9ALgQuNX2Y022jYiIdjWZrO8vKe5o+lOKO5luAi5rsN1CYKvthwAkXQ0sBu4fof85FOFzINtGRMQYazJZ31PAP5Sf/TEL2FZZHgJOrusoaQawCDh/f7eNiIh2jBoQkl4P/A3w62V/AbY92lxMde+McE0bFK8yvb0yAWDjbSUtBZYCzJ07d5SSIiKiqSanmC4HPkjxsNzeUfpWDQFzKsuzge0j9F3C06eX9mtb2yuAFQCdTmekAIqIiP3UJCB+YvurB7DvdcB8SfOAH1CEwLndnSQdCbwJeOf+bhsREe1pEhBfl3QRcD3wi+FG23f32sj2HknnU9wFNQ24wvYmScvK9ZeWXc8Gbhp+53WvbfdjXBER8SzJ7n1WRtLXa5pt+7R2SjpwnU7Hg4OD/S4jIuKgIWm97U7duiZ3Mf37sS8pIiImuiZ3MX24rt32R8e+nIiImCiaXIPYVfn9uRSvId3cTjkRETFRNDnF9KnqsqRPAqtbqygiIiaEJnMxdZsBjPaQXEREHOSaXIP4Nk8/xTwNGABy/SEiYpLr9U7qebYfprjmMGwP8CPbe1qvLCIi+qrXKaZry59X2P5++flBwiEiYmrodYrpOZI+Ahwv6c+7V9r+dHtlRUREv/U6glgC/CtFiLyg5hMREZPYiEcQtrcAH5d03wFO1hcREQexUW9zTThERExNB/IcRERETAFNptqY9P72f2/i/u1P9LuMiIgDcsKvHcFHznr5mO931CMISTMk/bWkfyqX50s6c7TtIiLi4NbkCOJzFK8bfV25PAR8GfhKW0WNtzaSNyLiYNfkGsRLbH8C+CWA7Z8DarWqiIjouyYBsVvS4ZTzMUl6CZVXj0ZExOTU5BTT3wBrgDmSrgJeD/xRizVFRMQE0OQ5iJuAt1OEwkqgY3ttk51LWiRpi6StkpaP0OdUSRskbZJ0a6X9Akkby/YPNPm+iIgYO02m+15NEQyrbe8arX9lu2nAJcAZFBe210labfv+Sp+ZwGeBRbYfkXRs2f4K4N3AQmA3sEbS/7H93cYji4iIZ6XJNYhPAacA90v6sqTfk/TcBtstBLbafsj2buBqYHFXn3OB620/AmB7R9n+MuBO2z8rZ4+9FTi7wXdGRMQYaXKK6Vbb76V4i9wK4PeBHb23AmAWsK2yPFS2VR0PHCVpraT1ks4r2zcCb5T0QkkzgLcCcxp8Z0REjJFGT1KXdzGdBbwDOAm4sslmNW3uWp4OvBo4HTgcuEPSnbY3S/o4cDPwU+BeipcV1dW2FFgKMHfu3AZlRUREE02epP6fwGbgNIprCi+x/f4G+x7imf/XPxvYXtNnje1dth8FbgNeBWD7ctsn2X4j8BhQe/3B9grbHdudgYGBBmVFREQTTa5BfI4iFJbZvsX2Uw33vQ6YL2mepEMp3i+xuqvPKuAUSdPLU0knU4QRlQvWcynuolrZ8HsjImIM9Hon9Wm2bwFmAIulZ54xsn19rx3b3iPpfOBGYBrFq0s3SVpWrr+0PJW0BrgPeAq4zPbGchfXSXohxRPc77P94wMbYkREHIhe1yDeBNxCce2hm4GeAQFg+5+Bf+5qu7Rr+SLgopptTxlt/xER0Z5eb5T7SPnrR20/XF0naV6rVUVERN81uQZxXU3btWNdSERETCy9rkH8BvBy4EhJb6+sOgJo8qBcREQcxHpdg3gpcCYwk2deh3iSYhqMiIiYxHpdg1gFrJL0Ott3jGNNERExATS5BrGsnFQPAElHSbqivZIiImIiaBIQr7T9+PBC+TzCia1VFBERE0KTgHiOpKOGFyQdTcM5nCIi4uDV5A/9p4BvSrqW4gG53wf+W6tVRURE340aELa/IGmQYrI+AW+vvvQnIiImpyanmACOBnbZ/gywM09SR0RMfk2m+/4I8JfAhWXTIcAX2ywqIiL6r8kRxNnA24BdALa3Ay9os6iIiOi/JgGx27Yp3wYn6XntlhQRERNBk4C4RtI/AjMlvRv4v8A/tVtWRET0W5O7mD4p6QzgCYr5mT5s++bWK4uIiL5q9MBbGQgJhYiIKWTEU0ySvlH+fFLSEzWfhyW9d/xKjYiI8dRrNtc3lD9r71gq3xf9TeCz7ZQWERH91OgUk6STgDdQ3Mn0Ddv32P5/kk5tsbaIiOijJg/KfRi4EnghcAzweUn/FcD2D0fZdpGkLZK2Slo+Qp9TJW2QtEnSrZX2D5ZtGyWtlJS32EVEjKMmRxDnACfa/lcASR8D7gb+rtdGkqYBlwBnAEPAOkmrq/M4le+Z+CywyPYjko4t22cBfwacYPvnkq4BlgCf37/hRUTEgWryHMT3eOY7qA8DHmyw3UJgq+2HbO8GrgYWd/U5F7je9iMAtndU1k0HDpc0HZgBbG/wnRERMUZGPIKQ9BmKaw6/ADZJurlcPgP4RoN9zwK2VZaHgJO7+hwPHCJpLcX0HRfb/oLtH0j6JPAI8HPgJts3NRtSRESMhV6nmAbLn+uBGyrtaxvuWzVtrvn+VwOnA4cDd0i6E9hJcbQxD3gc+LKkd9reZ5JASUuBpQBz585tWFpERIym122uVwKUF4f/LcUf9weHr0U0MATMqSzPZt/TREPAo7Z3Absk3Qa8qlz3sO2dZQ3XA79JzSyytlcAKwA6nU53AEVExAHq9aDcdEmfoPgjfiXFH+dtkj4h6ZAG+14HzJc0T9KhFBeZV3f1WQWcUn7XDIpTUJspTi29VtIMSaI4wti8v4OLiIgD1+sU00UU1wXm2X4SQNIRwCfLzwW9dmx7j6TzgRuBacAVtjdJWlauv9T2ZklrgPuAp4DLbG8sv+tairul9gD3UB4lRETE+FAxk3fNCum7wPHu6lDevvqA7fnjUN9+6XQ6HhwcHL1jREQAIGm97U7dul63ubo7HMrGvex7sTkiIiaZXgFxv6TzuhslvRN4oL2SIiJiIuh1DeJ9wPWS/oTiVlcDr6G4HfXscagtIiL6qNdtrj8ATpZ0GvByiucavmr7a+NVXERE9E+TN8rdAtwyDrVERMQE0mQupoiImIISEBERUSsBERERtRIQERFRKwERERG1EhAREVErAREREbUSEBERUSsBERERtRIQERFRKwERERG1EhAREVErAREREbUSEBERUavVgJC0SNIWSVslLR+hz6mSNkjaJOnWsu2lZdvw5wlJH2iz1oiIeKZR3wdxoCRNAy4BzgCGgHWSVtu+v9JnJvBZYJHtRyQdC2B7C7Cgsp8fADe0VWtEROyrzSOIhcBW2w/Z3g1cDSzu6nMucL3tRwBs76jZz+nAg7a/32KtERHRpc2AmAVsqywPlW1VxwNHSVorab2k82r2swRY2VKNERExgtZOMVG8w7qba77/1RRHCYcDd0i60/Z3ACQdCrwNuHDEL5GWAksB5s6dOwZlR0QEtHsEMQTMqSzPBrbX9Flje5ftR4HbgFdV1r8FuNv2j0b6EtsrbHdsdwYGBsao9IiIaDMg1gHzJc0rjwSWAKu7+qwCTpE0XdIM4GRgc2X9OeT0UkREX7R2isn2HknnAzcC04ArbG+StKxcf6ntzZLWAPcBTwGX2d4IUAbGGcB72qoxIiJGJrv7ssDBq9PpeHBwsN9lREQcNCStt92pW5cnqSMiolYCIiIiaiUgIiKiVgIiIiJqJSAiIqJWAiIiImolICIiolYCIiIiaiUgIiKiVgIiIiJqJSAiIqJWAiIiImolICIiolYCIiIiaiUgIiKiVgIiIiJqJSAiIqJWAiIiImolICIiolYCIiIiarUaEJIWSdoiaauk5SP0OVXSBkmbJN1aaZ8p6VpJD0jaLOl1bdYaERHPNL2tHUuaBlwCnAEMAeskrbZ9f6XPTOCzwCLbj0g6trKLi4E1tn9P0qHAjLZqjYiIfbV5BLEQ2Gr7Idu7gauBxV19zgWut/0IgO0dAJKOAN4IXF6277b9eIu1RkRElzYDYhawrbI8VLZVHQ8cJWmtpPWSzivbXwzsBD4n6R5Jl0l6Xou1RkRElzYDQjVt7lqeDrwa+B3gt4G/lnR82X4S8A+2TwR2ASNdw1gqaVDS4M6dO8es+IiIqa7NgBgC5lSWZwPba/qssb3L9qPAbcCryvYh23eV/a6lCIx92F5hu2O7MzAwMKYDiIiYytoMiHXAfEnzyovMS4DVXX1WAadImi5pBnAysNn2vwDbJL207Hc6cD8RETFuWruLyfYeSecDNwLTgCtsb5K0rFx/qe3NktYA9wFPAZfZ3lju4v3AVWW4PAT8cVu1RkTEvmR3XxY4eHU6HQ8ODva7jIiIg4ak9bY7devyJHVERNRKQERERK0ERERE1EpARERErQRERETUSkBEREStBERERNRKQERERK0ERERE1JpUT1JL2gl8/wA3PwZ4dAzLORhkzFNDxjw1HOiYf9127Uynkyogng1JgyM9bj5ZZcxTQ8Y8NbQx5pxiioiIWgmIiIiolYB42op+F9AHGfPUkDFPDWM+5lyDiIiIWjmCiIiIWlM+ICQtkrRF0lZJy/tdz7Mh6QpJOyRtrLQdLelmSd8tfx5VWXdhOe4tkn670v5qSd8u1/0PSRrvsTQlaY6kr0vaLGmTpAvK9kk7bknPlfQtSfeWY/7bsn3SjhlA0jRJ90j6Srk8qccLIOl7Zb0bJA2WbeM3bttT9kPxKtQHgRcDhwL3Aif0u65nMZ43AicBGyttnwCWl78vBz5e/n5COd7DgHnlP4dp5bpvAa8DBHwVeEu/x9ZjzC8CTip/fwHwnXJsk3bcZX3PL38/BLgLeO1kHnNZ658DXwK+MhX+3S7r/R5wTFfbuI17qh9BLAS22n7I9m7gamBxn2s6YLZvAx7ral4MXFn+fiXwu5X2q23/wvbDwFZgoaQXAUfYvsPFv1lfqGwz4dj+oe27y9+fBDYDs5jE43bhp+XiIeXHTOIxS5oN/A5wWaV50o53FOM27qkeELOAbZXlobJtMvk3tn8IxR9T4NiyfaSxzyp/726f8CQdB5xI8X/Uk3rc5emWDcAO4Gbbk33M/x34C+CpSttkHu8wAzdJWi9padk2buOe/iwKnwzqzsNNldu6Rhr7QfnPRNLzgeuAD9h+oscp1kkxbtt7gQWSZgI3SHpFj+4H9ZglnQnssL1e0qlNNqlpO2jG2+X1trdLOha4WdIDPfqO+bin+hHEEDCnsjwb2N6nWtryo/IQk/LnjrJ9pLEPlb93t09Ykg6hCIerbF9fNk/6cQPYfhxYCyxi8o759cDbJH2P4jTwaZK+yOQd76/Y3l7+3AHcQHFafNzGPdUDYh0wX9I8SYcCS4DVfa5prK0G/rD8/Q+BVZX2JZIOkzQPmA98qzxkfVLSa8s7Hc6rbDPhlDVeDmy2/enKqkk7bkkD5ZEDkg4H3gw8wCQds+0Lbc+2fRzFf6O32H4nk3S8wyQ9T9ILhn8HfgvYyHiOu99X6fv9Ad5KcefLg8Bf9bueZzmWlcAPgV9S/F/Du4AXAl8Dvlv+PLrS/6/KcW+hclcD0Cn/RXwQ+HvKByon4gd4A8Xh8n3AhvLz1sk8buCVwD3lmDcCHy7bJ+2YK/WeytN3MU3q8VLcXXlv+dk0/PdpPMedJ6kjIqLWVD/FFBERI0hARERErQRERETUSkBEREStBERERNRKQETUkPTT8udxks4d433/l67lb47l/iPGSgIiorfjgP0KCEnTRunyjICw/Zv7WVPEuEhARPT2MeCUcj7+D5aT5F0kaZ2k+yS9B0DSqSreS/El4Ntl2/8qJ1nbNDzRmqSPAYeX+7uqbBs+WlG5743l3P3vqOx7raRrJT0g6aqJ/h6DmBym+mR9EaNZDvxn22cClH/of2L7NZIOA26XdFPZdyHwChdTLQP8ie3Hyukw1km6zvZySefbXlDzXW8HFgCvAo4pt7mtXHci8HKKOXRup5if6BtjPdiIqhxBROyf3wLOK6favoti2oP55bpvVcIB4M8k3QvcSTGJ2nx6ewOw0vZe2z8CbgVeU9n3kO2nKKYTOW4MxhLRU44gIvaPgPfbvvEZjcU01Lu6lt8MvM72zyStBZ7bYN8j+UXl973kv90YBzmCiOjtSYpXmQ67EfjTcopxJB1fzrTZ7Ujgx2U4/AbFK0GH/XJ4+y63Ae8or3MMULxC9ltjMoqIA5D/C4no7T5gT3mq6PPAxRSnd+4uLxTvpP71jWuAZZLuo5hZ887KuhXAfZLutv0HlfYbKN4bfC/FDLV/YftfyoCJGHeZzTUiImrlFFNERNRKQERERK0ERERE1EpARERErQRERETUSkBEREStBERERNRKQERERK3/DwmNtElNVpw5AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lamb = 0\n",
    "n_iter = 5000\n",
    "n_batch = 1\n",
    "gamma = 0.05\n",
    "# gamma = lambda t: 1 / np.sqrt(t)\n",
    "\n",
    "mlr = MySGDLogisticRegression(gamma, n_iter, lamb, n_batch=n_batch, random_state=None)\n",
    "mlr.fit(X_train, y_train)\n",
    "print(\"Test accuracy\", mlr.score(X_test, y_test))\n",
    "\n",
    "obj_list = mlr.obj_list_\n",
    "iter_list = np.arange(len(obj_list)) * mlr.freq_obj_eval\n",
    "plt.plot(iter_list, obj_list)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Objective function\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.2 (private SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now implement and experiment with DP-SGD:\n",
    "1. Given the normalization applied to the data, what is the sensitivity of an individual gradient?\n",
    "2. Following the model of the function `sgd`, implement a new function `private_sgd` which implements DP-SGD **with mini-batch size of 1 and no regularization**. It can take as input the desired value of $\\epsilon$ and $\\delta$ for the $(\\epsilon,\\delta)$-DP, or alternatively the standard deviation of the Gaussian noise to add at each iteration. Note: you do not need to make the objective plotting part private (this is only for monitoring).\n",
    "3. Following the model of the class `MySGDLogisticRegression`, implement a new class `MyPrivateSGDLogisticRegression` which implements differentially private logistic regression trained using your DP-SGD implementation above.\n",
    "4. Experiment with different values of $\\epsilon$ and $\\delta$, number of iterations and step size, and study the effect on the convergence of SGD as well as the test accuracy of the resulting model. Describe your observations. How does the level of privacy affect the choice of the number of iterations? How can we choose the number of iterations and step size in practice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7ffb5ed3a4f0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lamb = 0\n",
    "n_iter = 500\n",
    "n_batch = 1\n",
    "gamma = 0.2\n",
    "# gamma = lambda t: 1 / np.sqrt(t)\n",
    "\n",
    "eps_list = np.linspace(0.1, 5.0, num=5)\n",
    "delta_list = [1. / n_train**2, 1. / n_train**4]\n",
    "n_runs = 10\n",
    "    \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "accuracy = np.zeros((len(eps_list), len(delta_list), n_runs))\n",
    "for j, delta in enumerate(delta_list):\n",
    "    for i, eps in enumerate(eps_list):\n",
    "        for r in range(n_runs):\n",
    "            # TO COMPLETE\n",
    "            pass\n",
    "            # accuracy[i, j, r] =\n",
    "    \n",
    "    ax.errorbar(eps_list, accuracy[:, j, :].mean(axis=1), accuracy[:, j, :].std(axis=1),\n",
    "                    label='DP-SGD ($\\delta$=' + \"{:.2e}\".format(delta) + ')')\n",
    "\n",
    "plt.xlabel(\"$\\epsilon$\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Bonus Question</font> (extending the scope of private SGD)\n",
    "*(You won't be penalized if you don't answer this question; but can get bonus points if you do)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider the following extensions, which were briefly discussed in the lecture:\n",
    "1. *$\\ell_2$-regularization*: What is the sensitivity of the stochastic gradient when adding $\\ell_2$-regularization to the objective (parameter `lamb` in the code above)? Adapt your code if needed.\n",
    "2. *Mini-batch*: What is the sensitivity of a stochastic gradient when it is evaluated on a mini-batch of $b$ data points (parameter `n_batch` in the code above)? Adapt your code if needed.\n",
    "3. *Gradient clipping*: If the loss function $L$ is not Lipschitz, or when the Lipschitz constant is difficult to bound, the idea of gradient clipping consists in rescaling each individual gradient that have a norm larger than some constant $C$ to have norm equal to $C$:\n",
    "$$\\text{clip}(\\nabla L(\\theta;x,y), C) = \\min\\Big(1,\\frac{C}{\\|\\nabla L(\\theta;x,y)\\|_2}\\Big)\\nabla L(\\theta;x,y)$$\n",
    "\n",
    "Explain how this allows to bound the gradient sensitivity without any assumption on the Lipschitzness of the loss. Implement this variant in a function `private_sgd_with_clipping` and explore how to choose the value of $C$ for logistic regression on the *unnormalized* version of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
